# **METHOD 4: COHESION AND SEPARATION (WSS/BSS) - COMPLETE GUIDE**

---

## **4.1 CONCEPT: SYNOPSIS**

**Cohesion and Separation** are two fundamental concepts that measure cluster quality from complementary perspectives:

**Cohesion (Within-Cluster Sum of Squares - WSS):**
```
Measures how tightly grouped points are within their own cluster
Lower WSS = Better cohesion = Compact clusters
```

**Separation (Between-Cluster Sum of Squares - BSS):**
```
Measures how distinct/separated clusters are from each other
Higher BSS = Better separation = Well-separated clusters
```

**Key Idea:**
```
Good clustering = Low WSS + High BSS
- Points close to their cluster centers (cohesion)
- Cluster centers far from overall center (separation)
```

**Fundamental Relationship:**
```
TSS = WSS + BSS

Where TSS (Total Sum of Squares) is constant for given data
```

**Visual Concept:**
```
GOOD CLUSTERING:              BAD CLUSTERING:
    â—â—â—         â—â—â—              â—  â—  â—  â—  â—
   â—â—â—â—â—       â—â—â—â—â—            â—  â—  â—  â—  â—
    â—â—â—         â—â—â—              â—  â—  â—  â—  â—

Low WSS (tight)             High WSS (spread)
High BSS (far apart)        Low BSS (close together)
```

---

## **4.2 MATHEMATICAL REASONING WITH PROOF**

### **4.2.1 Formal Definitions**

#### **Total Sum of Squares (TSS)**

**Definition:**
```
TSS = Î£ ||xáµ¢ - Î¼||Â²
      i=1 to n

Where:
- xáµ¢ = data point i
- Î¼ = overall mean (centroid of all data)
- ||Â·|| = Euclidean distance
- n = total number of points
```

**Physical Meaning:** Total variance in the dataset

---

#### **Within-Cluster Sum of Squares (WSS)**

**Definition:**
```
WSS = Î£   Î£   ||x - Î¼â±¼||Â²
      j=1 xÏµCâ±¼
      to k

Where:
- k = number of clusters
- Câ±¼ = cluster j
- Î¼â±¼ = centroid of cluster j
- x = point in cluster Câ±¼
```

**Alternative Notation:**
```
WSS = Î£ SSEâ±¼
      j=1 to k

Where SSEâ±¼ is the SSE of cluster j
```

**Physical Meaning:** 
```
Total variance WITHIN clusters
Measures compactness/cohesion
Same as SSE we studied earlier!
```

---

#### **Between-Cluster Sum of Squares (BSS)**

**Definition:**
```
BSS = Î£ nâ±¼Â·||Î¼â±¼ - Î¼||Â²
      j=1 to k

Where:
- k = number of clusters
- nâ±¼ = number of points in cluster j
- Î¼â±¼ = centroid of cluster j
- Î¼ = overall mean
```

**Physical Meaning:** 
```
Variance BETWEEN clusters
Measures separation
Weighted by cluster sizes (nâ±¼)
```

---

### **4.2.2 The Fundamental Theorem**

#### **Theorem: TSS = WSS + BSS**

**Statement:**
```
For any clustering of a dataset:
Total Sum of Squares = Within-Cluster SS + Between-Cluster SS

TSS = WSS + BSS
```

**This is the most important property of cohesion-separation metrics!**

---

**Proof:**

**Step 1: Expand TSS**
```
TSS = Î£ ||xáµ¢ - Î¼||Â²
      i=1 to n
```

**Step 2: Decompose by adding and subtracting cluster means**

For each point xáµ¢ in cluster Câ±¼:
```
||xáµ¢ - Î¼||Â² = ||xáµ¢ - Î¼â±¼ + Î¼â±¼ - Î¼||Â²
```

**Step 3: Expand the squared norm**
```
||xáµ¢ - Î¼â±¼ + Î¼â±¼ - Î¼||Â² = ||xáµ¢ - Î¼â±¼||Â² + ||Î¼â±¼ - Î¼||Â² + 2âŸ¨xáµ¢ - Î¼â±¼, Î¼â±¼ - Î¼âŸ©
```

**Step 4: Sum over all points in cluster Câ±¼**
```
Î£   ||xáµ¢ - Î¼||Â² = Î£   ||xáµ¢ - Î¼â±¼||Â² + Î£   ||Î¼â±¼ - Î¼||Â² + 2âŸ¨Î£  (xáµ¢ - Î¼â±¼), Î¼â±¼ - Î¼âŸ©
xáµ¢ÏµCâ±¼            xáµ¢ÏµCâ±¼                xáµ¢ÏµCâ±¼               xáµ¢ÏµCâ±¼
```

**Step 5: Simplify the cross term**

Since Î¼â±¼ is the mean of cluster Câ±¼:
```
Î£  (xáµ¢ - Î¼â±¼) = 0  (by definition of mean)
xáµ¢ÏµCâ±¼

Therefore, the cross term vanishes!
```

**Step 6: Continue simplification**
```
Î£   ||xáµ¢ - Î¼||Â² = Î£   ||xáµ¢ - Î¼â±¼||Â² + nâ±¼Â·||Î¼â±¼ - Î¼||Â²
xáµ¢ÏµCâ±¼            xáµ¢ÏµCâ±¼

Where nâ±¼ = |Câ±¼| (number of points in cluster j)
```

**Step 7: Sum over all clusters**
```
TSS = Î£   Î£   ||xáµ¢ - Î¼||Â²
      j=1 xáµ¢ÏµCâ±¼
      to k

    = Î£   [Î£   ||xáµ¢ - Î¼â±¼||Â² + nâ±¼Â·||Î¼â±¼ - Î¼||Â²]
      j=1  xáµ¢ÏµCâ±¼
      to k

    = Î£   Î£   ||xáµ¢ - Î¼â±¼||Â² + Î£ nâ±¼Â·||Î¼â±¼ - Î¼||Â²
      j=1 xáµ¢ÏµCâ±¼              j=1
      to k                   to k

    = WSS + BSS  âœ“  QED
```

---

**Geometric Interpretation:**

```
Total variance = Within-cluster variance + Between-cluster variance

Think of it like ANOVA:
- TSS: Total variation in data
- WSS: Variation within groups (unexplained by clustering)
- BSS: Variation between groups (explained by clustering)

Good clustering maximizes the ratio: BSS/TSS
This is the proportion of variance "explained" by clustering
```

---

### **4.2.3 Properties and Theorems**

#### **Property 1: Range and Bounds**

**Theorem:** For k clusters on n points:

```
0 â‰¤ WSS â‰¤ TSS
0 â‰¤ BSS â‰¤ TSS
WSS + BSS = TSS (always)
```

**Extreme Cases:**

**Case 1: k = 1 (single cluster)**
```
All points in one cluster
Î¼â±¼ = Î¼ (cluster mean = overall mean)

WSS = Î£ ||xáµ¢ - Î¼||Â² = TSS
BSS = nâ‚Â·||Î¼â‚ - Î¼||Â² = nÂ·||Î¼ - Î¼||Â² = 0

Check: WSS + BSS = TSS + 0 = TSS âœ“
```

**Case 2: k = n (each point its own cluster)**
```
Each cluster has one point
xáµ¢ = Î¼áµ¢ (point is its own centroid)

WSS = Î£ Î£ ||x - Î¼â±¼||Â² = 0 (no deviation from self)
BSS = Î£ 1Â·||xáµ¢ - Î¼||Â² = TSS

Check: WSS + BSS = 0 + TSS = TSS âœ“
```

---

#### **Property 2: Monotonicity**

**Theorem:** As k increases:
```
WSS decreases monotonically: WSS(k+1) â‰¤ WSS(k)
BSS increases monotonically: BSS(k+1) â‰¥ BSS(k)
```

**Proof:**
```
Given: TSS = constant

From TSS = WSS + BSS:
If WSS decreases â†’ BSS must increase (to maintain sum)

WSS decreases because:
- More clusters â†’ points closer to their centroids
- Same principle as SSE monotonicity

Therefore:
â†“ WSS âŸº â†‘ BSS
```

**Practical Implication:**
```
Cannot use WSS or BSS alone to determine optimal k
(both improve monotonically with k)

Must use ratios or combined metrics:
- BSS/WSS ratio
- BSS/TSS (proportion of variance explained)
- Compare with elbow method, silhouette, etc.
```

---

#### **Property 3: Scale Invariance**

**Theorem:** BSS/WSS ratio is scale-invariant

**Proof:**
```
Let data be scaled by factor Î» > 0:
x'áµ¢ = Î»Â·xáµ¢

Then:
Î¼' = Î»Â·Î¼
Î¼'â±¼ = Î»Â·Î¼â±¼

WSS' = Î£ Î£ ||Î»x - Î»Î¼â±¼||Â²
     = Î£ Î£ Î»Â²||x - Î¼â±¼||Â²
     = Î»Â²Â·WSS

BSS' = Î£ nâ±¼||Î»Î¼â±¼ - Î»Î¼||Â²
     = Î£ nâ±¼Â·Î»Â²||Î¼â±¼ - Î¼||Â²
     = Î»Â²Â·BSS

BSS'/WSS' = (Î»Â²Â·BSS)/(Î»Â²Â·WSS) = BSS/WSS âœ“

The ratio is preserved under scaling!
```

---

### **4.2.4 Relationship to Other Metrics**

#### **Connection to RÂ² (Coefficient of Determination)**

**Definition:**
```
RÂ² = BSS / TSS = 1 - WSS/TSS

Interpretation:
Proportion of variance explained by clustering
```

**Range:** 0 â‰¤ RÂ² â‰¤ 1

**Meaning:**
```
RÂ² = 0: Clustering explains no variance (k=1)
RÂ² = 1: Clustering explains all variance (k=n)
RÂ² = 0.8: Clustering explains 80% of variance
```

---

#### **Connection to F-Statistic**

**Calinski-Harabasz Index** uses WSS and BSS:
```
CH = [BSS/(k-1)] / [WSS/(n-k)]

This is analogous to F-statistic in ANOVA:
F = (Between-group variance) / (Within-group variance)
```

---

#### **Connection to Silhouette**

**Conceptual relationship:**
```
Silhouette measures:
- a(i): individual point's cohesion
- b(i): individual point's separation

WSS/BSS measures:
- WSS: aggregate cohesion across all points
- BSS: aggregate separation of clusters

Both capture similar concepts at different granularities:
- Silhouette: point-level
- WSS/BSS: cluster-level
```

---

## **4.3 CONCEPT EXPLAINED**

### **4.3.1 Intuitive Understanding**

**Cohesion (WSS) - "How tight are the clusters?"**

```
Imagine social groups at a party:

TIGHT GROUP (Low WSS):
People standing very close together
    ðŸ‘¥
   ðŸ‘¥ðŸ‘¥
    ðŸ‘¥
Everyone within arm's reach

LOOSE GROUP (High WSS):
People spread out across room
ðŸ‘¤    ðŸ‘¤    ðŸ‘¤
   ðŸ‘¤       ðŸ‘¤
ðŸ‘¤    ðŸ‘¤    ðŸ‘¤
Hard to tell who's in the group
```

---

**Separation (BSS) - "How distinct are the clusters?"**

```
Imagine groups in a room:

WELL-SEPARATED (High BSS):
Groups in different corners
    ðŸ‘¥         
   ðŸ‘¥ðŸ‘¥         
    ðŸ‘¥          
              ðŸ‘¥
             ðŸ‘¥ðŸ‘¥
              ðŸ‘¥
Clear group boundaries

OVERLAPPING (Low BSS):
Groups mixed together
    ðŸ‘¥ðŸ‘¥  
   ðŸ‘¥ðŸ‘¥ðŸ‘¥ðŸ‘¥
    ðŸ‘¥ðŸ‘¥ðŸ‘¥
     ðŸ‘¥ðŸ‘¥
Which group is which?
```

---

**Combined Quality:**

```
EXCELLENT CLUSTERING:
Tight groups + far apart
    â—â—â—              â—â—â—
   â—â—â—â—â—            â—â—â—â—â—
    â—â—â—              â—â—â—

Low WSS + High BSS = Best!

POOR CLUSTERING:
Loose groups + close together
  â—  â—  â—      â—  â—  â—
 â—  â—  â—  â—  â—  â—  â—  â—
  â—  â—  â—      â—  â—  â—

High WSS + Low BSS = Worst!
```

---

### **4.3.2 Step-by-Step Calculation Example**

**Simple 1D Example:**

```
Data: {1, 2, 4, 5}
Clustering: {1, 2} and {4, 5}
```

---

**Step 1: Calculate Overall Mean (Î¼)**

```
Î¼ = (1 + 2 + 4 + 5) / 4
  = 12 / 4
  = 3
```

---

**Step 2: Calculate TSS (Total Sum of Squares)**

```
TSS = Î£ (xáµ¢ - Î¼)Â²
    = (1-3)Â² + (2-3)Â² + (4-3)Â² + (5-3)Â²
    = (-2)Â² + (-1)Â² + (1)Â² + (2)Â²
    = 4 + 1 + 1 + 4
    = 10
```

---

**Step 3: Calculate Cluster Centroids**

```
Cluster 1: {1, 2}
Î¼â‚ = (1 + 2) / 2 = 1.5

Cluster 2: {4, 5}
Î¼â‚‚ = (4 + 5) / 2 = 4.5
```

---

**Step 4: Calculate WSS (Within-Cluster Sum of Squares)**

**Cluster 1:**
```
WSSâ‚ = (1-1.5)Â² + (2-1.5)Â²
     = (-0.5)Â² + (0.5)Â²
     = 0.25 + 0.25
     = 0.5
```

**Cluster 2:**
```
WSSâ‚‚ = (4-4.5)Â² + (5-4.5)Â²
     = (-0.5)Â² + (0.5)Â²
     = 0.25 + 0.25
     = 0.5
```

**Total WSS:**
```
WSS = WSSâ‚ + WSSâ‚‚
    = 0.5 + 0.5
    = 1.0
```

---

**Step 5: Calculate BSS (Between-Cluster Sum of Squares)**

```
BSS = nâ‚Â·(Î¼â‚ - Î¼)Â² + nâ‚‚Â·(Î¼â‚‚ - Î¼)Â²
    = 2Â·(1.5 - 3)Â² + 2Â·(4.5 - 3)Â²
    = 2Â·(-1.5)Â² + 2Â·(1.5)Â²
    = 2Â·2.25 + 2Â·2.25
    = 4.5 + 4.5
    = 9.0
```

---

**Step 6: Verify TSS = WSS + BSS**

```
WSS + BSS = 1.0 + 9.0 = 10.0 = TSS âœ“

Perfect! The theorem holds.
```

---

**Step 7: Calculate Quality Metrics**

**RÂ² (Variance Explained):**
```
RÂ² = BSS / TSS
   = 9.0 / 10.0
   = 0.90
   = 90%

Interpretation: Clustering explains 90% of variance!
```

**BSS/WSS Ratio:**
```
BSS / WSS = 9.0 / 1.0 = 9.0

Interpretation: Between-cluster variance is 9Ã— within-cluster variance
High ratio = Excellent clustering!
```

---

**Interpretation Summary:**

```
âœ“ WSS = 1.0 (LOW - clusters very compact)
âœ“ BSS = 9.0 (HIGH - clusters well-separated)
âœ“ RÂ² = 0.90 (90% variance explained)
âœ“ BSS/WSS = 9.0 (excellent ratio)

This is excellent clustering! The two groups are:
- Very tight (points close to cluster means)
- Very distinct (cluster means far from overall mean)
```

---

### **4.3.3 Complex 2D Example**

**Dataset:** 
```
Cluster 1: A(1,1), B(2,2), C(1,2), D(2,1)
Cluster 2: E(8,8), F(9,9), G(8,9), H(9,8)
```

---

**Step 1: Calculate Overall Mean**

```
Î¼_x = (1+2+1+2+8+9+8+9) / 8 = 40/8 = 5.0
Î¼_y = (1+2+2+1+8+9+9+8) / 8 = 40/8 = 5.0

Î¼ = (5.0, 5.0)
```

---

**Step 2: Calculate TSS**

```
For each point, calculate ||point - Î¼||Â²:

A: (1-5)Â² + (1-5)Â² = 16 + 16 = 32
B: (2-5)Â² + (2-5)Â² = 9 + 9 = 18
C: (1-5)Â² + (2-5)Â² = 16 + 9 = 25
D: (2-5)Â² + (1-5)Â² = 9 + 16 = 25
E: (8-5)Â² + (8-5)Â² = 9 + 9 = 18
F: (9-5)Â² + (9-5)Â² = 16 + 16 = 32
G: (8-5)Â² + (9-5)Â² = 9 + 16 = 25
H: (9-5)Â² + (8-5)Â² = 16 + 9 = 25

TSS = 32 + 18 + 25 + 25 + 18 + 32 + 25 + 25
    = 200
```

---

**Step 3: Calculate Cluster Centroids**

**Cluster 1:**
```
Î¼â‚_x = (1+2+1+2)/4 = 1.5
Î¼â‚_y = (1+2+2+1)/4 = 1.5
Î¼â‚ = (1.5, 1.5)
```

**Cluster 2:**
```
Î¼â‚‚_x = (8+9+8+9)/4 = 8.5
Î¼â‚‚_y = (8+9+9+8)/4 = 8.5
Î¼â‚‚ = (8.5, 8.5)
```

---

**Step 4: Calculate WSS**

**Cluster 1:**
```
A: (1-1.5)Â² + (1-1.5)Â² = 0.25 + 0.25 = 0.5
B: (2-1.5)Â² + (2-1.5)Â² = 0.25 + 0.25 = 0.5
C: (1-1.5)Â² + (2-1.5)Â² = 0.25 + 0.25 = 0.5
D: (2-1.5)Â² + (1-1.5)Â² = 0.25 + 0.25 = 0.5

WSSâ‚ = 0.5 + 0.5 + 0.5 + 0.5 = 2.0
```

**Cluster 2 (by symmetry):**
```
WSSâ‚‚ = 2.0
```

**Total WSS:**
```
WSS = 2.0 + 2.0 = 4.0
```

---

**Step 5: Calculate BSS**

```
BSS = nâ‚Â·||Î¼â‚ - Î¼||Â² + nâ‚‚Â·||Î¼â‚‚ - Î¼||Â²

||Î¼â‚ - Î¼||Â² = (1.5-5)Â² + (1.5-5)Â²
            = (-3.5)Â² + (-3.5)Â²
            = 12.25 + 12.25
            = 24.5

||Î¼â‚‚ - Î¼||Â² = (8.5-5)Â² + (8.5-5)Â²
            = (3.5)Â² + (3.5)Â²
            = 12.25 + 12.25
            = 24.5

BSS = 4Â·24.5 + 4Â·24.5
    = 98 + 98
    = 196
```

---

**Step 6: Verify**

```
WSS + BSS = 4.0 + 196 = 200 = TSS âœ“
```

---

**Step 7: Quality Metrics**

```
RÂ² = BSS/TSS = 196/200 = 0.98 = 98%

BSS/WSS = 196/4.0 = 49

Interpretation:
- 98% of variance explained by clustering! â­
- Between-cluster variance is 49Ã— within-cluster variance!
- Exceptional clustering quality!
```

---

## **4.4 METHOD: STEP-BY-STEP ALGORITHM**

### **ALGORITHM:**

```
INPUT:
  - Data points: X = {xâ‚, xâ‚‚, ..., xâ‚™}
  - Cluster assignments: labels = {Lâ‚, Lâ‚‚, ..., Lâ‚™}
  - Number of clusters: k

OUTPUT:
  - WSS (Within-Cluster Sum of Squares)
  - BSS (Between-Cluster Sum of Squares)
  - TSS (Total Sum of Squares)
  - RÂ² (Variance explained)
  - BSS/WSS ratio

ALGORITHM:

STEP 1: Calculate Overall Mean
    Î¼ = (1/n) Ã— Î£ xáµ¢
                i=1 to n

STEP 2: Calculate TSS
    TSS = Î£ ||xáµ¢ - Î¼||Â²
          i=1 to n

STEP 3: For each cluster j from 1 to k:
    
    3.1: Identify points in cluster Câ±¼
    
    3.2: Calculate cluster centroid
         Î¼â±¼ = (1/nâ±¼) Ã— Î£ x
                       xâˆˆCâ±¼
    
    3.3: Calculate WSS for this cluster
         WSSâ±¼ = Î£ ||x - Î¼â±¼||Â²
                xâˆˆCâ±¼
    
    3.4: Calculate BSS contribution
         BSSâ±¼ = nâ±¼ Ã— ||Î¼â±¼ - Î¼||Â²

STEP 4: Aggregate results
    WSS = Î£ WSSâ±¼
          j=1 to k
    
    BSS = Î£ BSSâ±¼
          j=1 to k

STEP 5: Verify and calculate metrics
    5.1: Verify: TSS = WSS + BSS (should hold)
    5.2: Calculate RÂ² = BSS / TSS
    5.3: Calculate ratio = BSS / WSS

STEP 6: Return WSS, BSS, TSS, RÂ², ratio

COMPLEXITY:
Time: O(nÂ·dÂ·k)
  - Calculate overall mean: O(nÂ·d)
  - Calculate TSS: O(nÂ·d)
  - For each cluster: O(nâ±¼Â·d)
  - Total cluster work: O(nÂ·d)
  - Overall: O(nÂ·d)

Space: O(kÂ·d)
  - Store k cluster centroids
  - Each centroid has d dimensions
```

---

### **DETAILED PSEUDOCODE:**

```python
FUNCTION calculate_wss_bss(X, labels, k):
    """
    Calculate Within and Between Cluster Sum of Squares
    
    Args:
        X: data matrix [n Ã— d]
        labels: cluster assignments [n]
        k: number of clusters
    
    Returns:
        WSS, BSS, TSS, R_squared, BSS_WSS_ratio
    """
    n = NUMBER_OF_ROWS(X)
    d = NUMBER_OF_COLUMNS(X)
    
    # Step 1: Calculate overall mean
    Î¼ = MEAN(X, axis=0)  # Mean across all points
    
    # Step 2: Calculate TSS
    TSS = 0
    FOR i IN 0 TO n-1:
        diff = X[i] - Î¼
        TSS += DOT_PRODUCT(diff, diff)  # ||diff||Â²
    
    # Step 3: Initialize aggregates
    WSS = 0
    BSS = 0
    
    # Step 4: For each cluster
    FOR j IN 0 TO k-1:
        # Get points in this cluster
        cluster_mask = (labels == j)
        cluster_points = X[cluster_mask]
        nâ±¼ = NUMBER_OF_ROWS(cluster_points)
        
        IF nâ±¼ == 0:
            CONTINUE  # Empty cluster
        
        # Calculate cluster centroid
        Î¼â±¼ = MEAN(cluster_points, axis=0)
        
        # Calculate WSS for this cluster
        WSSâ±¼ = 0
        FOR each point x IN cluster_points:
            diff = x - Î¼â±¼
            WSSâ±¼ += DOT_PRODUCT(diff, diff)
        WSS += WSSâ±¼
        
        # Calculate BSS contribution
        centroid_diff = Î¼â±¼ - Î¼
        BSSâ±¼ = nâ±¼ Ã— DOT_PRODUCT(centroid_diff, centroid_diff)
        BSS += BSSâ±¼
    
    # Step 5: Verify theorem
    IF ABS((WSS + BSS) - TSS) > 0.001:
        PRINT("Warning: TSS â‰  WSS + BSS")
    
    # Step 6: Calculate metrics
    R_squared = BSS / TSS
    BSS_WSS_ratio = BSS / WSS IF WSS > 0 ELSE INFINITY
    
    RETURN WSS, BSS, TSS, R_squared, BSS_WSS_ratio


FUNCTION compare_clusterings(X, labels_list, k_list):
    """
    Compare multiple clusterings using WSS/BSS metrics
    
    Args:
        X: data matrix
        labels_list: list of cluster assignments
        k_list: list of k values
    
    Returns:
        comparison_table
    """
    results = []
    
    FOR i IN 0 TO LENGTH(labels_list)-1:
        labels = labels_list[i]
        k = k_list[i]
        
        WSS, BSS, TSS, RÂ², ratio = calculate_wss_bss(X, labels, k)
        
        results.APPEND({
            'k': k,
            'WSS': WSS,
            'BSS': BSS,
            'TSS': TSS,
            'RÂ²': RÂ²,
            'BSS/WSS': ratio
        })
    
    RETURN results
```

---

## **4.5 DEMONSTRATE METHOD WITH EXAMPLES**

### **EXAMPLE 1: Complete Calculation (From PPT Slide)**

**Problem (From PPT):**

Given data points {1, 2, 4, 5} with two clusterings:

**Clustering:** {1, 2} and {4, 5}

Calculate:
- WSS (Within-Cluster SS)
- BSS (Between-Cluster SS)
- Verify TSS = WSS + BSS
- Calculate BSS/WSS ratio

---

#### **COMPLETE SOLUTION:**

**Given:**
```
Data: {1, 2, 4, 5}
n = 4 points
k = 2 clusters
Cluster 1: {1, 2}
Cluster 2: {4, 5}
```

---

**Step 1: Calculate Overall Mean**

```
Î¼ = (1 + 2 + 4 + 5) / 4
  = 12 / 4
  = 3
```

---

**Step 2: Calculate TSS**

```
TSS = Î£ (xáµ¢ - Î¼)Â²
    = (1-3)Â² + (2-3)Â² + (4-3)Â² + (5-3)Â²
    = 4 + 1 + 1 + 4
    = 10
```

---

**Step 3: Calculate Cluster Centroids**

**Cluster 1: {1, 2}**
```
nâ‚ = 2
Î¼â‚ = (1 + 2) / 2 = 1.5
```

**Cluster 2: {4, 5}**
```
nâ‚‚ = 2
Î¼â‚‚ = (4 + 5) / 2 = 4.5
```

---

**Step 4: Calculate WSS**

**For Cluster 1:**
```
WSSâ‚ = (1 - 1.5)Â² + (2 - 1.5)Â²
     = (-0.5)Â² + (0.5)Â²
     = 0.25 + 0.25
     = 0.5
```

**For Cluster 2:**
```
WSSâ‚‚ = (4 - 4.5)Â² + (5 - 4.5)Â²
     = (-0.5)Â² + (0.5)Â²
     = 0.25 + 0.25
     = 0.5
```

**Total WSS:**
```
WSS = WSSâ‚ + WSSâ‚‚
    = 0.5 + 0.5
    = 1.0
```

---

**Step 5: Calculate BSS**

**Formula:**
```
BSS = nâ‚Â·(Î¼â‚ - Î¼)Â² + nâ‚‚Â·(Î¼â‚‚ - Î¼)Â²
```

**For Cluster 1:**
```
BSSâ‚ = nâ‚ Ã— (Î¼â‚ - Î¼)Â²
     = 2 Ã— (1.5 - 3)Â²
     = 2 Ã— (-1.5)Â²
     = 2 Ã— 2.25
     = 4.5
```

**For Cluster 2:**
```
BSSâ‚‚ = nâ‚‚ Ã— (Î¼â‚‚ - Î¼)Â²
     = 2 Ã— (4.5 - 3)Â²
     = 2 Ã— (1.5)Â²
     = 2 Ã— 2.25
     = 4.5
```

**Total BSS:**
```
BSS = BSSâ‚ + BSSâ‚‚
    = 4.5 + 4.5
    = 9.0
```

---

**Step 6: Verify TSS = WSS + BSS**

```
WSS + BSS = 1.0 + 9.0 = 10.0

TSS = 10.0

10.0 = 10.0 âœ“ VERIFIED!

The fundamental theorem holds.
```

---

**Step 7: Calculate Quality Metrics**

**RÂ² (Proportion of Variance Explained):**
```
RÂ² = BSS / TSS
   = 9.0 / 10.0
   = 0.90
   = 90%
```

**BSS/WSS Ratio:**
```
BSS / WSS = 9.0 / 1.0
          = 9.0
```

---

**Summary Table:**

| Metric | Value | Interpretation |
|--------|-------|----------------|
| TSS | 10.0 | Total variance |
| WSS | 1.0 | Low (tight clusters) âœ“ |
| BSS | 9.0 | High (well-separated) âœ“ |
| RÂ² | 0.90 | 90% variance explained âœ“ |
| BSS/WSS | 9.0 | Excellent ratio âœ“ |

---

**INTERPRETATION:**

```
âœ“ WSS = 1.0 (10% of total variance)
  - Clusters are very compact
  - Points close to their cluster means
  
âœ“ BSS = 9.0 (90% of total variance)
  - Clusters are well-separated
  - Cluster means far from overall mean
  
âœ“ RÂ² = 0.90
  - Clustering "explains" 90% of variance
  - Only 10% due to within-cluster spread
  
âœ“ BSS/WSS = 9.0
  - Between-cluster variance is 9Ã— within-cluster
  - Rule of thumb: ratio > 5 is excellent
  
CONCLUSION: This is excellent clustering! â­
The two groups {1,2} and {4,5} are:
- Very tight internally (low WSS)
- Very distinct from each other (high BSS)
- Natural structure with gap from 2 to 4
```

---

### **EXAMPLE 2: Comparing Two Clusterings (From PPT)**

**Problem:**

Given the same data {1, 2, 4, 5}, compare:

**Clustering A:** {1, 2} and {4, 5}  [Natural clustering]
**Clustering B:** {1, 4} and {2, 5}  [Unnatural clustering]

Which clustering is better based on WSS/BSS metrics?

---

#### **COMPLETE SOLUTION:**

**For Clustering A:** [Already calculated above]
```
WSS_A = 1.0
BSS_A = 9.0
RÂ²_A = 0.90
```

---

**For Clustering B:** {1, 4} and {2, 5}

**Step 1: Calculate Cluster Centroids**

```
Cluster 1: {1, 4}
Î¼â‚ = (1 + 4) / 2 = 2.5

Cluster 2: {2, 5}
Î¼â‚‚ = (2 + 5) / 2 = 3.5

Overall mean: Î¼ = 3 (same as before)
TSS = 10 (same as before - doesn't depend on clustering)
```

---

**Step 2: Calculate WSS_B**

**For Cluster 1: {1, 4}**
```
WSSâ‚ = (1 - 2.5)Â² + (4 - 2.5)Â²
     = (-1.5)Â² + (1.5)Â²
     = 2.25 + 2.25
     = 4.5
```

**For Cluster 2: {2, 5}**
```
WSSâ‚‚ = (2 - 3.5)Â² + (5 - 3.5)Â²
     = (-1.5)Â² + (1.5)Â²
     = 2.25 + 2.25
     = 4.5
```

**Total WSS_B:**
```
WSS_B = 4.5 + 4.5 = 9.0
```

---

**Step 3: Calculate BSS_B**

```
BSSâ‚ = 2 Ã— (2.5 - 3)Â²
     = 2 Ã— (-0.5)Â²
     = 2 Ã— 0.25
     = 0.5

BSSâ‚‚ = 2 Ã— (3.5 - 3)Â²
     = 2 Ã— (0.5)Â²
     = 2 Ã— 0.25
     = 0.5

BSS_B = 0.5 + 0.5 = 1.0
```

---

**Step 4: Verify**

```
WSS_B + BSS_B = 9.0 + 1.0 = 10.0 = TSS âœ“
```

---

**Step 5: Calculate Metrics for Clustering B**

```
RÂ²_B = BSS_B / TSS
     = 1.0 / 10.0
     = 0.10
     = 10%

BSS_B/WSS_B = 1.0 / 9.0
            = 0.111
```

---

**COMPARISON TABLE:**

| Metric | Clustering A | Clustering B | Winner |
|--------|-------------|-------------|--------|
| WSS | 1.0 âœ“ | 9.0 | A (lower is better) |
| BSS | 9.0 âœ“ | 1.0 | A (higher is better) |
| RÂ² | 0.90 âœ“ | 0.10 | A (higher is better) |
| BSS/WSS | 9.0 âœ“ | 0.111 | A (higher is better) |

---

**INTERPRETATION:**

**Clustering A: {1,2} and {4,5}**
```
âœ“ WSS = 1.0 (LOW - tight clusters)
âœ“ BSS = 9.0 (HIGH - well-separated)
âœ“ RÂ² = 0.90 (explains 90% of variance)
âœ“ BSS/WSS = 9.0 (excellent ratio)

Quality: EXCELLENT
- Points close to their cluster centers
- Clusters far from each other
- Matches natural data structure (gap from 2 to 4)
```

**Clustering B: {1,4} and {2,5}**
```
âœ— WSS = 9.0 (HIGH - loose clusters)
âœ— BSS = 1.0 (LOW - poorly separated)
âœ— RÂ² = 0.10 (explains only 10% of variance)
âœ— BSS/WSS = 0.111 (poor ratio)

Quality: POOR
- Points far from their cluster centers
- Clusters close to each other
- Forces unnatural groupings
- Cluster centers (2.5, 3.5) both near overall mean (3)
```

---

**Visual Comparison:**

```
Data: 1â”€â”€â”€2       4â”€â”€â”€5

Clustering A (Good):
      [â”€â”€â”€]       [â”€â”€â”€]
      C1=1.5      C2=4.5
      
      Low WSS: Points near centers
      High BSS: Centers far from Î¼=3

Clustering B (Bad):
      1       [â”€] 
              C1=2.5
          2       [â”€]
                  C2=3.5
                      4
                          5
      
      High WSS: Points far from centers (1 far from 2.5, 4 far from 2.5)
      Low BSS: Centers near Î¼=3
```

---

**CONCLUSION:**

**Clustering A is dramatically better!**

```
All metrics favor Clustering A:
- 9Ã— lower WSS
- 9Ã— higher BSS
- 9Ã— higher RÂ²
- 81Ã— higher BSS/WSS ratio

Clustering A captures the natural structure:
- Natural gap in data (2â†’4)
- Adjacent points grouped
- Makes intuitive sense

Clustering B violates natural structure:
- Splits adjacent pairs
- Mixes distant pairs
- Makes no intuitive sense
```

---
# **METHOD 4: COHESION AND SEPARATION (WSS/BSS) - CONTINUED**

---

## **4.6 HOW IS WSS/BSS AFFECTED BY VARIOUS FACTORS?**

### **FACTOR 1: Number of Clusters (k)**

#### **Effect:**
```
As k increases:
- WSS decreases monotonically (always)
- BSS increases monotonically (always)
- RÂ² increases toward 1
- But optimal k is where rate of change slows
```

#### **Mathematical Explanation:**

```
For k clusters:
TSS = WSS(k) + BSS(k) = constant

Since TSS is fixed:
If WSSâ†“ then BSSâ†‘ (must sum to TSS)

Extreme cases:
k=1: WSS = TSS, BSS = 0, RÂ² = 0
k=n: WSS = 0, BSS = TSS, RÂ² = 1
```

---

#### **Detailed Example:**

**Dataset:** {1, 2, 3, 10, 11, 12, 20, 21, 22}
**TSS = 548** (constant regardless of k)

---

**k=1: All points in one cluster**

```
Cluster: {1,2,3,10,11,12,20,21,22}
Î¼ = 11.33

WSS(k=1) = Î£(xáµ¢ - 11.33)Â²
         = 548

BSS(k=1) = 9Â·(11.33 - 11.33)Â²
         = 0

RÂ²(k=1) = 0/548 = 0%
```

---

**k=2: Split into two groups**

```
Clusters: {1,2,3,10,11,12} and {20,21,22}
Î¼â‚ = 6.5, Î¼â‚‚ = 21

WSS(k=2) = 125.5 + 2 = 127.5  â† Decreased from 548
BSS(k=2) = 6Ã—(6.5-11.33)Â² + 3Ã—(21-11.33)Â²
         = 139.4 + 280.9
         = 420.3  â† Increased from 0

RÂ²(k=2) = 420.3/548 = 76.7%

Verify: 127.5 + 420.3 = 547.8 â‰ˆ 548 âœ“
```

---

**k=3: Natural clusters**

```
Clusters: {1,2,3}, {10,11,12}, {20,21,22}
Î¼â‚ = 2, Î¼â‚‚ = 11, Î¼â‚ƒ = 21

WSS(k=3) = 2 + 2 + 2 = 6  â† Large decrease!
BSS(k=3) = 3Ã—(2-11.33)Â² + 3Ã—(11-11.33)Â² + 3Ã—(21-11.33)Â²
         = 261.3 + 0.3 + 280.1
         = 541.7

RÂ²(k=3) = 541.7/548 = 98.9%  â† High!

Verify: 6 + 541.7 = 547.7 â‰ˆ 548 âœ“
```

---

**k=4: Over-clustering**

```
Clusters: {1,2,3}, {10,11}, {12}, {20,21,22}

WSS(k=4) â‰ˆ 4.5  â† Small decrease
BSS(k=4) â‰ˆ 543.5

RÂ²(k=4) = 99.2%  â† Marginally better

Diminishing returns!
```

---

**k=5, 6, 7, ...**

```
WSS continues decreasing: 3.0, 2.5, 2.0, ...
BSS continues increasing: 545, 545.5, 546, ...
RÂ² approaches 1: 99.4%, 99.5%, 99.6%, ...

But improvements become tiny!
```

---

**Summary Plot:**

```
WSS                           BSS
 â”‚                             â”‚
548â”‚â€¢                         548â”‚              â€¢
   â”‚ â•²                          â”‚            â•±
   â”‚  â•²                         â”‚          â•±
127â”‚   â€¢                     420â”‚        â€¢
   â”‚    â•²___                    â”‚      â•±
  6â”‚        â€¢â”€â”€â”€ k=3 optimal 542â”‚    â€¢â”€â”€â”€â”€â”€ k=3 optimal
   â”‚         â•²___                â”‚  â•±
  0â”‚            â€¢â”€â”€â”€â”€â”€â”€â€¢â”€â”€â€¢    0â”‚ â€¢
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ k    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ k
   1   2   3   4   5   6        1   2   3   4   5   6
   
   Sharp elbow at k=3         Sharp bend at k=3
```

**RÂ² Plot:**

```
RÂ²
  â”‚
1.0â”‚                      â€¢â”€â”€â”€â€¢
   â”‚                    â•±
   â”‚                  â•±
   â”‚                â€¢  â† k=3 (98.9%)
   â”‚              â•±
   â”‚            â•±
   â”‚          â€¢
0.5â”‚        â•±
   â”‚      â•±
   â”‚    â€¢
   â”‚  â•±
0.0â”‚â€¢
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ k
   1   2   3   4   5   6
   
   Elbow at k=3
```

---

**Key Insight:**

```
Like SSE, WSS/BSS metrics are monotonic with k
Cannot directly pick "best" k from WSS or BSS alone

Must use rate of change analysis:
- Look for elbow in WSS plot
- Or elbow in RÂ² plot
- Or use ratio BSS/WSS with elbow method

Best practice:
- Plot RÂ² vs k
- Find k where RÂ² reaches ~90-95%
- Consider elbow point (diminishing returns)
```

---

### **FACTOR 2: Data Separation (Natural Clustering)**

#### **Effect:**
```
Well-separated data â†’ High BSS/WSS ratio at optimal k
Overlapping data â†’ Low BSS/WSS ratio even at optimal k
```

#### **Example:**

**Dataset A: Well-Separated**
```
Group 1: {1, 2, 3}      at x â‰ˆ 2
Group 2: {20, 21, 22}   at x â‰ˆ 21

Gap: 17 units (very large)

For k=2:
WSS â‰ˆ 2 (very low - tight groups)
BSS â‰ˆ 700 (very high - far apart)
BSS/WSS â‰ˆ 350 (excellent!)
RÂ² â‰ˆ 99.7%
```

**Dataset B: Overlapping**
```
Group 1: {1, 2, 3, 4, 5}     at x â‰ˆ 3
Group 2: {4, 5, 6, 7, 8}     at x â‰ˆ 6

Overlap: Points 4, 5 in both regions

For k=2:
WSS â‰ˆ 15 (higher - spread groups)
BSS â‰ˆ 25 (lower - closer together)
BSS/WSS â‰ˆ 1.67 (poor)
RÂ² â‰ˆ 62.5%
```

---

**Visual Comparison:**

```
WELL-SEPARATED (High BSS/WSS):

â—â—â—                            â—â—â—
â—â—â—                            â—â—â—
â—â—â—                            â—â—â—
â†â”€â”€â”€ Large gap (17 units) â”€â”€â”€â”€â†’

WSS: Points tightly clustered (small)
BSS: Clusters far apart (large)
Result: High BSS/WSS ratio


OVERLAPPING (Low BSS/WSS):

â—â—â—â—â—
  â—â—â—â—â—
    â—â—â—â—â—

WSS: Points spread within clusters (larger)
BSS: Clusters close together (smaller)
Result: Low BSS/WSS ratio
```

---

### **FACTOR 3: Cluster Size Imbalance**

#### **Effect:**
```
Balanced clusters â†’ BSS depends on all clusters equally
Imbalanced clusters â†’ BSS dominated by large clusters
```

#### **Mathematical Explanation:**

```
BSS = Î£ nâ±¼Â·||Î¼â±¼ - Î¼||Â²
      j=1 to k

The weight nâ±¼ means larger clusters contribute more to BSS!
```

---

#### **Example:**

**Balanced Clustering:**
```
Cluster 1: 50 points at x=10
Cluster 2: 50 points at x=20
Overall Î¼ = 15

BSS = 50Ã—(10-15)Â² + 50Ã—(20-15)Â²
    = 50Ã—25 + 50Ã—25
    = 1250 + 1250
    = 2500

Both clusters contribute equally
```

**Imbalanced Clustering:**
```
Cluster 1: 90 points at x=10
Cluster 2: 10 points at x=20
Overall Î¼ = (90Ã—10 + 10Ã—20)/100 = 11

BSS = 90Ã—(10-11)Â² + 10Ã—(20-11)Â²
    = 90Ã—1 + 10Ã—81
    = 90 + 810
    = 900

Cluster 2 contributes 810/900 = 90% of BSS
despite having only 10% of points!

Small cluster at extreme position dominates BSS
```

---

**Impact on Metrics:**

```
For imbalanced clusters:
- BSS can be artificially inflated by small outlier clusters
- WSS may be low overall but high in large cluster
- RÂ² can be misleading

Example:
Cluster 1: 100 points, tightly clustered (WSS=10)
Cluster 2: 5 points, scattered outliers (WSS=100)

Total WSS = 110
But 91% of WSS comes from 5% of points!

Need to look at per-cluster metrics:
- Average WSS per point in each cluster
- Weighted metrics
- Per-cluster RÂ²
```

---

### **FACTOR 4: Cluster Shape**

#### **Effect:**
```
Spherical/compact clusters â†’ Low WSS, high BSS/WSS
Elongated/spread clusters â†’ High WSS, lower BSS/WSS
```

#### **Example:**

**Spherical Clusters:**
```
Cluster 1: Points uniformly in circle, radius=1, center=(0,0)
Cluster 2: Points uniformly in circle, radius=1, center=(10,0)

For Cluster 1:
Average distance to center â‰ˆ 0.67 (for uniform circle)
WSSâ‚ â‰ˆ nâ‚ Ã— 0.67Â² â‰ˆ 0.45nâ‚

For Cluster 2:
WSSâ‚‚ â‰ˆ 0.45nâ‚‚

BSS = nâ‚Ã—(0-5)Â² + nâ‚‚Ã—(10-5)Â²
    = 25nâ‚ + 25nâ‚‚ (assuming equal sizes)

BSS/WSS â‰ˆ 50n / 0.9n â‰ˆ 55.6 (excellent!)
```

**Elongated Clusters:**
```
Cluster 1: Points along line from (0,0) to (5,0)
Cluster 2: Points along line from (10,0) to (15,0)

For Cluster 1:
Centroid at (2.5, 0)
Average distance to centroid â‰ˆ 2.5/âˆš3 â‰ˆ 1.44
WSSâ‚ â‰ˆ nâ‚ Ã— 1.44Â² â‰ˆ 2.07nâ‚

For Cluster 2:
WSSâ‚‚ â‰ˆ 2.07nâ‚‚

BSS = nâ‚Ã—(2.5-7.5)Â² + nâ‚‚Ã—(12.5-7.5)Â²
    = 25nâ‚ + 25nâ‚‚

BSS/WSS â‰ˆ 50n / 4.14n â‰ˆ 12.1 (lower)
```

**Comparison:**
```
Spherical: BSS/WSS â‰ˆ 55.6
Elongated: BSS/WSS â‰ˆ 12.1

Same separation, but elongated shape increases WSS
â†’ Lower BSS/WSS ratio
â†’ Appears worse even though separation is same
```

---

### **FACTOR 5: Outliers**

#### **Effect:**
```
Outliers increase WSS dramatically (if in a cluster)
Outliers increase BSS if they form their own cluster
```

#### **Example:**

**Clean Data:**
```
Cluster 1: {1, 2, 3}
Cluster 2: {10, 11, 12}

WSS = 2 + 2 = 4
BSS = ... = high
BSS/WSS = high (excellent)
```

**With Outlier in Cluster:**
```
Cluster 1: {1, 2, 3, 50}  â† outlier added!
Cluster 2: {10, 11, 12}

Î¼â‚ = (1+2+3+50)/4 = 14 (pulled by outlier)

WSSâ‚ = (1-14)Â² + (2-14)Â² + (3-14)Â² + (50-14)Â²
     = 169 + 144 + 121 + 1296
     = 1730  â† HUGE increase!

Total WSS â‰ˆ 1730 + 2 = 1732
BSS/WSS = very low (poor)
```

**Outlier as Separate Cluster:**
```
Cluster 1: {1, 2, 3}
Cluster 2: {10, 11, 12}
Cluster 3: {50}  â† outlier isolated

WSSâ‚ = 2
WSSâ‚‚ = 2
WSSâ‚ƒ = 0 (singleton)
Total WSS = 4 (same as clean!)

But BSS increases (more clusters)
BSS/WSS = higher

However, k increased to 3 for essentially 2 natural groups
```

---

**Key Insight:**

```
Outliers affect WSS/BSS differently depending on treatment:

1. If forced into nearest cluster:
   â†’ Increases WSS dramatically
   â†’ Lowers BSS/WSS ratio
   â†’ Makes clustering appear poor

2. If isolated in singleton cluster:
   â†’ WSS stays low (outlier has WSS=0)
   â†’ BSS increases
   â†’ But artificially increases k
   
Best practice:
- Remove outliers before clustering
- Or use robust clustering methods (K-medoids, DBSCAN)
- Or analyze per-cluster WSS to identify contaminated clusters
```

---

### **FACTOR 6: Dimensionality**

#### **Effect:**
```
Higher dimensions â†’ Higher absolute WSS and BSS values
But BSS/WSS ratio less affected
RÂ² remains comparable
```

#### **Mathematical Reason:**

```
In d dimensions:
WSS = Î£ Î£ Î£ (xáµ¢â±¼â‚– - Î¼â±¼â‚–)Â²
      j  i  k=1 to d

Each additional dimension adds to the sum
â†’ WSS and BSS both scale with d
â†’ But ratio BSS/WSS is more stable
```

---

#### **Example:**

**1D Data:**
```
Cluster 1: {1, 2, 3}
Cluster 2: {10, 11, 12}

WSS = 4
BSS = 162
BSS/WSS = 40.5
```

**2D Data (adding constant second dimension):**
```
Cluster 1: {(1,5), (2,5), (3,5)}
Cluster 2: {(10,5), (11,5), (12,5)}

First dimension contributes same as 1D:
WSS_x = 4, BSS_x = 162

Second dimension (constant=5):
WSS_y = 0, BSS_y = 0

Total:
WSS = 4 (same)
BSS = 162 (same)
BSS/WSS = 40.5 (same!)
```

**2D Data (both dimensions vary):**
```
Cluster 1: {(1,1), (2,2), (3,3)}
Cluster 2: {(10,10), (11,11), (12,12)}

If pattern is similar in both dimensions:
WSS â‰ˆ 4 + 4 = 8 (doubled)
BSS â‰ˆ 162 + 162 = 324 (doubled)
BSS/WSS â‰ˆ 40.5 (preserved!)
```

---

**Key Insight:**

```
Absolute values of WSS and BSS scale with dimensionality
But relative measures are more stable:
- BSS/WSS ratio: scale-invariant
- RÂ² = BSS/TSS: proportion, so scale-invariant

This is why we prefer ratios over absolute values!

However, in very high dimensions (d > 50):
- Curse of dimensionality affects distances
- All points become "far" from all others
- WSS and BSS both increase
- Clustering becomes less meaningful
```

---

### **FACTOR 7: Data Scale/Normalization**

#### **Effect:**
```
Unscaled features â†’ Dominant features control WSS/BSS
Scaled features â†’ All features contribute equally
```

#### **Example:**

**Unscaled Data:**
```
Feature 1 (Income): {20000, 25000, 80000, 85000}
Feature 2 (Age):    {25, 30, 40, 45}

For k=2: {(20000,25), (25000,30)} vs {(80000,40), (85000,45)}

WSS contribution from Income:
Cluster 1: (20000-22500)Â² + (25000-22500)Â² = 6.25M + 6.25M = 12.5M
Cluster 2: (80000-82500)Â² + (85000-82500)Â² = 6.25M + 6.25M = 12.5M
Total: 25M

WSS contribution from Age:
Cluster 1: (25-27.5)Â² + (30-27.5)Â² = 6.25 + 6.25 = 12.5
Cluster 2: (40-42.5)Â² + (45-42.5)Â² = 6.25 + 6.25 = 12.5
Total: 25

Total WSS â‰ˆ 25,000,025
Income dominates (contributes 99.9999%)!
Age essentially ignored!
```

**Scaled Data (z-score normalization):**
```
After scaling each feature to mean=0, std=1:

Feature 1: {-1.15, -0.38, 0.38, 1.15}
Feature 2: {-1.15, -0.38, 0.38, 1.15}

Now both features contribute equally to WSS/BSS
```

---

**Key Insight:**

```
Always normalize features before calculating WSS/BSS!

Common normalization methods:
1. Z-score: (x - Î¼) / Ïƒ
2. Min-Max: (x - min) / (max - min)
3. Decimal scaling: x / 10^k

Without normalization:
- Large-scale features dominate
- Small-scale features ignored
- Clustering driven by wrong features
- Misleading WSS/BSS values
```

---

### **FACTOR 8: Initialization (for K-means)**

#### **Effect:**
```
Good initialization â†’ Better local minimum â†’ Better WSS/BSS
Poor initialization â†’ Worse local minimum â†’ Worse WSS/BSS
```

#### **Example:**

**Good Initialization (K-means++):**
```
Dataset: {1,2,3,10,11,12,20,21,22}
Initial centroids: {2, 11, 21} (near natural centers)

Converges to:
Clusters: {1,2,3}, {10,11,12}, {20,21,22}
WSS = 6 (optimal)
BSS = 542 (optimal)
RÂ² = 98.9%
```

**Poor Initialization:**
```
Initial centroids: {1, 2, 3} (all in first group!)

Might converge to:
Clusters: {1,2}, {3,10,11}, {12,20,21,22}

WSS â‰ˆ 45 (suboptimal)
BSS â‰ˆ 503 (suboptimal)
RÂ² = 91.8% (worse)
```

---

**Detection:**

```
Unlike SSE alone, WSS/BSS metrics reveal poor initialization:

Multiple runs with different initializations:
Run 1: RÂ² = 98.9% âœ“
Run 2: RÂ² = 98.9% âœ“
Run 3: RÂ² = 91.8% âœ— (got stuck!)
Run 4: RÂ² = 98.9% âœ“

Low RÂ² indicates local minimum!

Best practice:
- Run K-means multiple times (10-50 runs)
- Keep clustering with highest RÂ² (or BSS/WSS)
- Or use K-means++ initialization
```

---

### **SUMMARY TABLE: FACTORS AFFECTING WSS/BSS**

| Factor | Effect on WSS | Effect on BSS | Effect on BSS/WSS | Mitigation |
|--------|---------------|---------------|-------------------|------------|
| **Increasing k** | â†“ Monotonic | â†‘ Monotonic | Varies (has optimum) | Use elbow on RÂ² plot |
| **Well-separated data** | â†“â†“ Very low | â†‘â†‘ Very high | â†‘â†‘ Very high | - |
| **Overlapping data** | â†‘ Higher | â†“ Lower | â†“ Lower | May not be clusterable |
| **Balanced clusters** | Depends on tightness | All clusters contribute | Fair assessment | - |
| **Imbalanced clusters** | Dominated by large | Dominated by large | Can be misleading | Analyze per-cluster |
| **Spherical clusters** | â†“ Low | â†‘ High | â†‘ High | - |
| **Elongated clusters** | â†‘ High | Moderate | â†“ Lower | Consider other algorithms |
| **Outliers in cluster** | â†‘â†‘ Very high | â†“ Lower | â†“â†“ Very low | Remove outliers first |
| **Outlier as singleton** | â†” Unchanged | â†‘ Increases | â†‘ Can increase | Better to remove |
| **High dimensions** | â†‘ Scales with d | â†‘ Scales with d | â†” Ratio stable | Use ratios (RÂ², BSS/WSS) |
| **Unscaled features** | Biased | Biased | Biased | **Always normalize!** |
| **Good initialization** | â†“ Optimal | â†‘ Optimal | â†‘ Optimal | Use K-means++ |
| **Poor initialization** | â†‘ Suboptimal | â†“ Suboptimal | â†“ Suboptimal | Multiple runs |

---

## **4.7 COMPARISON WITH OTHER CLUSTER VALIDATION METHODS**

### **4.7.1 WSS/BSS vs SSE (Sum of Squared Error)**

| Aspect | WSS | SSE | BSS |
|--------|-----|-----|-----|
| **What it measures** | Within-cluster variance | Same as WSS! | Between-cluster variance |
| **Relationship** | WSS = SSE | WSS â‰¡ SSE | BSS = TSS - WSS |
| **Range** | [0, TSS] | [0, TSS] | [0, TSS] |
| **Optimal value** | Minimize | Minimize | Maximize |
| **Considers separation** | No | No | Yes |
| **Complete picture** | No (only cohesion) | No (only cohesion) | No (only separation) |

**Key Point:** WSS and SSE are **exactly the same thing**!
```
WSS = Î£ Î£ ||x - Î¼â±¼||Â² = SSE
      j  xâˆˆCâ±¼

Different names for the same concept:
- SSE: Emphasizes "error" from cluster center
- WSS: Emphasizes "within-cluster" variance
```

---

#### **Advantage of WSS/BSS Framework:**

**SSE Alone:**
```
Only tells you about cohesion (compactness)
Cannot distinguish:

Case A: Tight clusters, well-separated
        SSE = 10

Case B: Tight clusters, overlapping
        SSE = 10

Same SSE, but very different quality!
```

**WSS + BSS:**
```
Tells complete story:

Case A:
  WSS = 10 (tight)
  BSS = 990 (well-separated)
  RÂ² = 99% (excellent!)

Case B:
  WSS = 10 (tight)
  BSS = 50 (overlapping)
  RÂ² = 83% (good but not great)

Now we can distinguish!
```

---

### **4.7.2 WSS/BSS vs SILHOUETTE COEFFICIENT**

| Aspect | WSS/BSS | Silhouette |
|--------|---------|------------|
| **Granularity** | Cluster-level | Point-level |
| **Measures** | Aggregate variance | Individual quality |
| **Cohesion** | WSS (global) | a(i) (per point) |
| **Separation** | BSS (global) | b(i) (per point) |
| **Computational cost** | O(nÂ·d) | O(nÂ²Â·d) |
| **Outlier detection** | No (aggregated) | Yes (negative s(i)) |
| **Interpretability** | Requires understanding variance | Very intuitive [-1,1] |
| **Best use** | Quick overall assessment | Detailed point analysis |

---

#### **Conceptual Relationship:**

```
Silhouette for point i:
s(i) = (b(i) - a(i)) / max{a(i), b(i)}

a(i) â‰ˆ point's contribution to WSS
b(i) â‰ˆ point's distance from other clusters (relates to separation)

Average silhouette across all points:
Highly correlated with BSS/WSS ratio!

Both measure similar concepts at different scales:
- Silhouette: micro-level (per point)
- WSS/BSS: macro-level (per cluster)
```

---

#### **When They Agree:**

**Example: Excellent Clustering**
```
Data: {1,2,3,10,11,12}
Clustering: {1,2,3} vs {10,11,12}

WSS/BSS metrics:
  RÂ² = 98%
  BSS/WSS = 49
  â†’ Excellent!

Silhouette:
  All points: s(i) > 0.85
  Average: 0.87
  â†’ Excellent!

Both methods agree! âœ“
```

---

#### **When They Disagree:**

**Example: Outlier Case**
```
Data: {1,2,3,10,11,12,100}
Clustering: {1,2,3,10,11,12} vs {100}

WSS/BSS metrics:
  WSS = 125.5 + 0 = 125.5
  BSS â‰ˆ 400
  RÂ² = 76%
  â†’ Looks decent

Silhouette:
  Point 100: s = 0 (singleton)
  Points near 12: s â‰ˆ 0.4 (affected by outlier)
  â†’ Reveals problem! âœ“

Silhouette is more sensitive to outliers
```

---

#### **Best Practice:**

```
Use both together:

1. WSS/BSS for quick global assessment:
   - Fast computation
   - Overall RÂ² value
   - Good for comparing k values

2. Silhouette for detailed analysis:
   - Identify problematic points
   - Detect outliers
   - Validate individual cluster quality

Workflow:
WSS/BSS â†’ Quick screening (RÂ² > 80%?)
   â†“ Yes
Silhouette â†’ Detailed validation (any s(i) < 0?)
   â†“ No
Accept clustering âœ“
```

---

### **4.7.3 WSS/BSS vs CALINSKI-HARABASZ (CH) INDEX**

**Recall CH Index:**
```
CH = [BSS/(k-1)] / [WSS/(n-k)]
```

**Relationship:**
```
CH Index directly uses WSS and BSS!

CH is essentially a normalized BSS/WSS ratio:
- Normalized by degrees of freedom
- (k-1) for between-cluster
- (n-k) for within-cluster
```

---

| Aspect | BSS/WSS Ratio | CH Index |
|--------|--------------|----------|
| **Formula** | BSS / WSS | [BSS/(k-1)] / [WSS/(n-k)] |
| **Normalization** | None | By degrees of freedom |
| **Statistical basis** | None | F-statistic analogy |
| **Comparison across datasets** | Not directly | Yes (more comparable) |
| **Comparison across k** | Somewhat | Better (accounts for k) |
| **Interpretation** | Ratio of variances | Pseudo F-statistic |

---

#### **Example:**

**Dataset:** {1,2,3,10,11,12}
**k=2:** {1,2,3} vs {10,11,12}

```
n = 6 points
WSS = 2
BSS = 42

Simple ratio:
BSS/WSS = 42/2 = 21

CH Index:
CH = [BSS/(k-1)] / [WSS/(n-k)]
   = [42/(2-1)] / [2/(6-2)]
   = [42/1] / [2/4]
   = 42 / 0.5
   = 84

CH is higher because it accounts for:
- k=2 (1 degree of freedom between clusters)
- n-k=4 (4 degrees of freedom within clusters)
```

---

**Comparing different k:**

```
k=2:
  BSS/WSS = 21
  CH = 84

k=3 (forced over-clustering):
  BSS/WSS = 25 (higher!)
  CH = 62.5 (lower)

CH decreases because:
- k increased (more degrees of freedom used)
- Improvement in BSS/WSS doesn't justify extra cluster
- Penalizes unnecessary complexity

CH is better for comparing across k values!
```

---

### **4.7.4 RÂ² (BSS/TSS) vs OTHER METRICS**

**RÂ² Formula:**
```
RÂ² = BSS / TSS = 1 - WSS/TSS
```

**Properties:**
- Range: [0, 1]
- Proportion of variance explained
- Analogous to regression RÂ²
- Monotonically increases with k

---

| Metric | Range | Monotonic with k | Intuitive Scale | Statistical Basis |
|--------|-------|------------------|-----------------|-------------------|
| **RÂ²** | [0, 1] | Yes â†‘ | Yes (%) | Yes (proportion) |
| **BSS/WSS** | [0, âˆž) | No (has optimum) | Somewhat | No |
| **Silhouette** | [-1, 1] | No (has optimum) | Very | No |
| **CH Index** | [0, âˆž) | No (has optimum) | No | Yes (F-stat) |
| **Gap Stat** | (-âˆž, âˆž) | No | No | Yes (statistical test) |

---

#### **When to Use RÂ²:**

**Advantages:**
```
âœ“ Easy to interpret (percentage)
âœ“ Comparable to regression RÂ²
âœ“ Standard statistical measure
âœ“ Scale: 0-100%
```

**Disadvantages:**
```
âœ— Monotonic (always increases with k)
âœ— Doesn't determine optimal k alone
âœ— Need elbow method on RÂ² curve
```

---

**Example Interpretation:**

```
RÂ² = 0.95 (95%)
"Clustering explains 95% of the total variance"

Similar to regression:
"Independent variables explain 95% of dependent variable variance"

Typical guidelines:
RÂ² > 90%: Excellent clustering
RÂ² 70-90%: Good clustering
RÂ² 50-70%: Moderate clustering
RÂ² < 50%: Weak clustering

But depends on context and data characteristics!
```

---

### **4.7.5 COMPREHENSIVE COMPARISON TABLE**

| Method | Cohesion | Separation | Combined | Per-Point | Optimal k | Speed | Best For |
|--------|----------|------------|----------|-----------|-----------|-------|----------|
| **SSE/WSS** | âœ“ | âœ— | âœ— | âœ— | No (monotonic) | âš¡âš¡âš¡ | Quick cohesion check |
| **BSS** | âœ— | âœ“ | âœ— | âœ— | No (monotonic) | âš¡âš¡âš¡ | Quick separation check |
| **RÂ² (BSS/TSS)** | âœ— | âœ“ | âœ“ | âœ— | With elbow | âš¡âš¡âš¡ | Overall quality % |
| **BSS/WSS** | âœ— | âœ— | âœ“ | âœ— | With elbow | âš¡âš¡âš¡ | Quality ratio |
| **CH Index** | âœ— | âœ— | âœ“ | âœ— | Yes (max) | âš¡âš¡âš¡ | Comparing k values |
| **Silhouette** | âœ“ | âœ“ | âœ“ | âœ“ | Yes (max) | âš¡âš¡ | Detailed analysis |
| **Gap Statistic** | âœ— | âœ— | âœ“ | âœ— | Yes (1-SE) | âš¡ | Statistical rigor |

---

### **4.7.6 DECISION MATRIX**

```
                Need per-point detail?
                    /          \
                 YES            NO
                  |              |
            Silhouette       Need speed?
                             /        \
                          YES          NO
                           |            |
                    WSS/BSS/RÂ²     Need stat rigor?
                       CH Index       /        \
                                   YES          NO
                                    |            |
                               Gap Stat      Silhouette
                                             (aggregate)
```

---

## **4.8 PYQ QUESTIONS AND DETAILED SOLUTIONS**

### **PYQ QUESTION 1: Basic WSS/BSS Calculation (6 marks)**

**Question (Based on PPT Example):**

Given data points {1, 2, 4, 5} clustered as:
- Cluster 1: {1, 2}
- Cluster 2: {4, 5}

Calculate:
(a) WSS (Within-cluster sum of squares) [2 marks]
(b) BSS (Between-cluster sum of squares) [2 marks]
(c) Verify that WSS + BSS = TSS [1 mark]
(d) Calculate RÂ² and interpret [1 mark]

---

#### **COMPLETE SOLUTION:**

**Given:**
```
Data: {1, 2, 4, 5}
n = 4
k = 2
Cluster 1: {1, 2}
Cluster 2: {4, 5}
```

---

**(a) Calculate WSS [2 marks]**

**Step 1: Calculate cluster centroids**
```
Î¼â‚ = (1 + 2) / 2 = 1.5
Î¼â‚‚ = (4 + 5) / 2 = 4.5
```

**Step 2: Calculate WSS for each cluster**

**Cluster 1:**
```
WSSâ‚ = (1 - 1.5)Â² + (2 - 1.5)Â²
     = (-0.5)Â² + (0.5)Â²
     = 0.25 + 0.25
     = 0.5
```

**Cluster 2:**
```
WSSâ‚‚ = (4 - 4.5)Â² + (5 - 4.5)Â²
     = (-0.5)Â² + (0.5)Â²
     = 0.25 + 0.25
     = 0.5
```

**Total WSS:**
```
WSS = WSSâ‚ + WSSâ‚‚
    = 0.5 + 0.5
    = 1.0
```

**ANSWER: WSS = 1.0** âœ“

[2 marks: 1 for cluster centroids, 1 for calculation]

---

**(b) Calculate BSS [2 marks]**

**Step 1: Calculate overall mean**
```
Î¼ = (1 + 2 + 4 + 5) / 4
  = 12 / 4
  = 3
```

**Step 2: Calculate BSS**

**Formula:**
```
BSS = Î£ nâ±¼Â·(Î¼â±¼ - Î¼)Â²
      j=1 to k
```

**For Cluster 1:**
```
nâ‚ = 2
BSSâ‚ = 2 Ã— (1.5 - 3)Â²
     = 2 Ã— (-1.5)Â²
     = 2 Ã— 2.25
     = 4.5
```

**For Cluster 2:**
```
nâ‚‚ = 2
BSSâ‚‚ = 2 Ã— (4.5 - 3)Â²
     = 2 Ã— (1.5)Â²
     = 2 Ã— 2.25
     = 4.5
```

**Total BSS:**
```
BSS = BSSâ‚ + BSSâ‚‚
    = 4.5 + 4.5
    = 9.0
```

**ANSWER: BSS = 9.0** âœ“

[2 marks: 1 for overall mean, 1 for calculation]

---

**(c) Verify WSS + BSS = TSS [1 mark]**

**Calculate TSS:**
```
TSS = Î£ (xáµ¢ - Î¼)Â²
    = (1-3)Â² + (2-3)Â² + (4-3)Â² + (5-3)Â²
    = 4 + 1 + 1 + 4
    = 10
```

**Verify:**
```
WSS + BSS = 1.0 + 9.0 = 10.0

TSS = 10.0

10.0 = 10.0 âœ“ VERIFIED!

The fundamental theorem TSS = WSS + BSS holds.
```

[1 mark for verification]

---

**(d) Calculate RÂ² and interpret [1 mark]**

**Calculate RÂ²:**
```
RÂ² = BSS / TSS
   = 9.0 / 10.0
   = 0.90
   = 90%
```

**Interpretation:**
```
RÂ² = 0.90 means:
- Clustering explains 90% of the total variance
- Only 10% of variance is within-cluster (WSS)
- 90% of variance is between-cluster (BSS)

This indicates excellent clustering quality!

The two clusters are:
âœ“ Very compact (low WSS = 10% of total)
âœ“ Well-separated (high BSS = 90% of total)
âœ“ Natural structure captured effectively
```

**ANSWER: RÂ² = 0.90 (90% variance explained) - Excellent clustering** âœ“

[1 mark for calculation and interpretation]

---

### **PYQ QUESTION 2: Comparing Two Clusterings (10 marks)**

**Question (Comprehensive comparison):**

Given dataset {2, 3, 4, 10, 11, 12}, compare the following two clusterings using WSS, BSS, and RÂ²:

**Clustering A:**
- Cluster 1: {2, 3, 4}
- Cluster 2: {10, 11, 12}

**Clustering B:**
- Cluster 1: {2, 3}
- Cluster 2: {4, 10, 11}
- Cluster 3: {12}

(a) For Clustering A, calculate WSS, BSS, and RÂ² [4 marks]
(b) For Clustering B, calculate WSS, BSS, and RÂ² [4 marks]
(c) Which clustering is better? Justify using the metrics [2 marks]

---

#### **COMPLETE SOLUTION:**

**Given:**
```
Data: {2, 3, 4, 10, 11, 12}
n = 6 points
```

**First, calculate TSS (same for both clusterings):**
```
Overall mean: Î¼ = (2+3+4+10+11+12)/6 = 42/6 = 7

TSS = (2-7)Â² + (3-7)Â² + (4-7)Â² + (10-7)Â² + (11-7)Â² + (12-7)Â²
    = 25 + 16 + 9 + 9 + 16 + 25
    = 100
```

---

**(a) Clustering A Analysis [4 marks]**

**Clustering A:** {2,3,4} and {10,11,12}

**Step 1: Calculate cluster centroids [1 mark]**
```
Cluster 1: {2, 3, 4}
Î¼â‚ = (2+3+4)/3 = 9/3 = 3

Cluster 2: {10, 11, 12}
Î¼â‚‚ = (10+11+12)/3 = 33/3 = 11
```

---

**Step 2: Calculate WSS [1 mark]**

**Cluster 1:**
```
WSSâ‚ = (2-3)Â² + (3-3)Â² + (4-3)Â²
     = 1 + 0 + 1
     = 2
```

**Cluster 2:**
```
WSSâ‚‚ = (10-11)Â² + (11-11)Â² + (12-11)Â²
     = 1 + 0 + 1
     = 2
```

**Total:**
```
WSS_A = 2 + 2 = 4
```

---

**Step 3: Calculate BSS [1 mark]**
```
BSS_A = nâ‚Â·(Î¼â‚-Î¼)Â² + nâ‚‚Â·(Î¼â‚‚-Î¼)Â²
      = 3Â·(3-7)Â² + 3Â·(11-7)Â²
      = 3Â·16 + 3Â·16
      = 48 + 48
      = 96
```

---

**Step 4: Calculate RÂ² [1 mark]**
```
RÂ²_A = BSS_A / TSS
     = 96 / 100
     = 0.96
     = 96%
```

**Verify:**
```
WSS_A + BSS_A = 4 + 96 = 100 = TSS âœ“
```

**Summary for Clustering A:**
```
WSS = 4
BSS = 96
RÂ² = 96%
```

[4 marks total]

---

**(b) Clustering B Analysis [4 marks]**

**Clustering B:** {2,3}, {4,10,11}, {12}

**Step 1: Calculate cluster centroids [1 mark]**
```
Cluster 1: {2, 3}
Î¼â‚ = (2+3)/2 = 2.5

Cluster 2: {4, 10, 11}
Î¼â‚‚ = (4+10+11)/3 = 25/3 = 8.33

Cluster 3: {12}
Î¼â‚ƒ = 12
```

---

**Step 2: Calculate WSS [1 mark]**

**Cluster 1:**
```
WSSâ‚ = (2-2.5)Â² + (3-2.5)Â²
     = 0.25 + 0.25
     = 0.5
```

**Cluster 2:**
```
WSSâ‚‚ = (4-8.33)Â² + (10-8.33)Â² + (11-8.33)Â²
     = (-4.33)Â² + (1.67)Â² + (2.67)Â²
     = 18.75 + 2.79 + 7.13
     = 28.67
```

**Cluster 3:**
```
WSSâ‚ƒ = 0  (singleton cluster)
```

**Total:**
```
WSS_B = 0.5 + 28.67 + 0 = 29.17
```

---

**Step 3: Calculate BSS [1 mark]**
```
BSS_B = nâ‚Â·(Î¼â‚-Î¼)Â² + nâ‚‚Â·(Î¼â‚‚-Î¼)Â² + nâ‚ƒÂ·(Î¼â‚ƒ-Î¼)Â²
      = 2Â·(2.5-7)Â² + 3Â·(8.33-7)Â² + 1Â·(12-7)Â²
      = 2Â·20.25 + 3Â·1.77 + 1Â·25
      = 40.5 + 5.31 + 25
      = 70.81
```

---

**Step 4: Calculate RÂ² [1 mark]**
```
RÂ²_B = BSS_B / TSS
     = 70.81 / 100
     = 0.708
     = 70.8%
```

**Verify:**
```
WSS_B + BSS_B = 29.17 + 70.81 = 100 â‰ˆ TSS âœ“
```

**Summary for Clustering B:**
```
WSS = 29.17
BSS = 70.81
RÂ² = 70.8%
```

[4 marks total]

---

**(c) Which clustering is better? [2 marks]**

**Comparison Table:**

| Metric | Clustering A | Clustering B | Winner |
|--------|-------------|-------------|--------|
| WSS | 4.0 âœ“ | 29.17 | A (lower is better) |
| BSS | 96.0 âœ“ | 70.81 | A (higher is better) |
| RÂ² | 96% âœ“ | 70.8% | A (higher is better) |
| BSS/WSS | 24.0 âœ“ | 2.43 | A (higher is better) |

---

**Detailed Justification:**

**1. WSS Comparison [0.5 marks]:**
```
Clustering A: WSS = 4.0
Clustering B: WSS = 29.17

WSS_A is 7.3Ã— lower than WSS_B!

Lower WSS means tighter, more cohesive clusters.

Clustering A: Both clusters very compact
Clustering B: Middle cluster {4,10,11} is spread out
             (spans gap from 4 to 11 - unnatural!)
```

---

**2. BSS Comparison [0.5 marks]:**
```
Clustering A: BSS = 96.0
Clustering B: BSS = 70.81

BSS_A is 1.36Ã— higher than BSS_B

Higher BSS means better separated clusters.

Clustering A: Cluster centers at 3 and 11 (8 units apart)
Clustering B: Centers at 2.5, 8.33, 12 (closer together)
```

---

**3. RÂ² Comparison [0.5 marks]:**
```
Clustering A: RÂ² = 96%
Clustering B: RÂ² = 70.8%

Clustering A explains 96% of variance
Clustering B explains only 70.8% of variance

25.2% difference in variance explained!

This is substantial - Clustering A is clearly superior.
```

---

**4. Qualitative Analysis [0.5 marks]:**

**Clustering A: {2,3,4} vs {10,11,12}**
```
âœ“ Respects natural structure (gap from 4 to 10)
âœ“ Adjacent numbers grouped together
âœ“ Both clusters have 3 points (balanced)
âœ“ Both clusters equally compact
âœ“ Makes intuitive sense
```

**Clustering B: {2,3}, {4,10,11}, {12}**
```
âœ— Violates natural structure
âœ— Middle cluster {4,10,11} spans the gap
âœ— Forces 4 (close to 2,3) with 10,11 (far away)
âœ— Creates singleton cluster (wasteful)
âœ— Imbalanced sizes: 2, 3, 1 points
âœ— Makes no intuitive sense
```

---

**CONCLUSION:**

**Clustering A is clearly superior! [Full marks]**

**Reasons:**
1. WSS 7.3Ã— lower (much tighter clusters)
2. BSS 36% higher (better separated)
3. RÂ² 25% higher (96% vs 71%)
4. BSS/WSS ratio 10Ã— higher (24.0 vs 2.4)
5. Respects natural data structure
6. Balanced cluster sizes
7. Intuitive and interpretable

**Clustering B fails because:**
- Unnatural middle cluster spanning gap
- 7Ã— higher within-cluster variance
- Only explains 71% of variance
- Poor cohesion and separation

**Recommendation: Use Clustering A** âœ“

[2 marks: 1 for comparison, 1 for detailed justification]

---

### **PYQ QUESTION 3: RÂ² and Optimal k (8 marks)**

**Question (Based on practical scenario):**

A data analyst performs K-means clustering on a customer dataset with different values of k and obtains the following results:

| k | WSS | BSS |
|---|-----|-----|
| 1 | 1000 | 0 |
| 2 | 450 | 550 |
| 3 | 150 | 850 |
| 4 | 130 | 870 |
| 5 | 115 | 885 |
| 6 | 105 | 895 |

(a) Calculate RÂ² for each value of k [3 marks]
(b) Plot RÂ² vs k and identify the elbow point [2 marks]
(c) Calculate the percentage increase in RÂ² for each increment of k [2 marks]
(d) Recommend optimal k with justification [1 mark]

---

#### **COMPLETE SOLUTION:**

**(a) Calculate RÂ² for each k [3 marks]**

**First, verify TSS:**
```
For any k: TSS = WSS + BSS

From k=1: TSS = 1000 + 0 = 1000 âœ“

Check others:
k=2: 450 + 550 = 1000 âœ“
k=3: 150 + 850 = 1000 âœ“
...

TSS = 1000 (constant for all k)
```

**Calculate RÂ² for each k:**

**Formula:** RÂ² = BSS / TSS

```
RÂ²(k=1) = 0 / 1000 = 0.000 = 0%

RÂ²(k=2) = 550 / 1000 = 0.550 = 55.0%

RÂ²(k=3) = 850 / 1000 = 0.850 = 85.0%

RÂ²(k=4) = 870 / 1000 = 0.870 = 87.0%

RÂ²(k=5) = 885 / 1000 = 0.885 = 88.5%

RÂ²(k=6) = 895 / 1000 = 0.895 = 89.5%
```

**Summary Table:**

| k | WSS | BSS | RÂ² |
|---|-----|-----|----|
| 1 | 1000 | 0 | 0% |
| 2 | 450 | 550 | 55.0% |
| 3 | 150 | 850 | 85.0% |
| 4 | 130 | 870 | 87.0% |
| 5 | 115 | 885 | 88.5% |
| 6 | 105 | 895 | 89.5% |

[3 marks: 0.5 marks per RÂ² calculation]

---

**(b) Plot RÂ² vs k and identify elbow [2 marks]**

**RÂ² vs k Plot:**

```
RÂ²
  â”‚
90%â”‚                          â€¢ (k=6)
   â”‚                        â•±
   â”‚                      â€¢   (k=5)
   â”‚                    â•±
87%â”‚                  â€¢       (k=4)
   â”‚                â•±
   â”‚              â•±
85%â”‚            â€¢             (k=3) â† ELBOW!
   â”‚          â•±
   â”‚        â•±
   â”‚      â•±
55%â”‚    â€¢                     (k=2)
   â”‚  â•±
   â”‚â•±
0% â”‚â€¢                         (k=1)
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ k
   1   2   3   4   5   6
```

**Analysis:**

```
The plot shows two distinct phases:

Phase 1 (k=1â†’3): Steep increase
- k=1â†’2: Jump of 55%
- k=2â†’3: Jump of 30%
- Rapid improvement in RÂ²

Phase 2 (k=3â†’6): Gradual increase
- k=3â†’4: Jump of 2%
- k=4â†’5: Jump of 1.5%
- k=5â†’6: Jump of 1%
- Diminishing returns

The ELBOW occurs at k=3!
```

[2 marks: 1 for plot, 1 for identifying elbow]

---

**(c) Calculate percentage increase for each increment [2 marks]**

**Formula:**
```
Percentage increase = [(RÂ²_new - RÂ²_old) / RÂ²_old] Ã— 100%
Or simply: RÂ²_new - RÂ²_old (absolute increase)
```

**Calculations:**

| Transition | RÂ² Old | RÂ² New | Absolute Increase | Relative Increase |
|------------|--------|--------|------------------|-------------------|
| k=1â†’2 | 0% | 55.0% | +55.0% | âˆž (from 0) |
| k=2â†’3 | 55.0% | 85.0% | +30.0% | +54.5% |
| k=3â†’4 | 85.0% | 87.0% | +2.0% | +2.4% |
| k=4â†’5 | 87.0% | 88.5% | +1.5% | +1.7% |
| k=5â†’6 | 88.5% | 89.5% | +1.0% | +1.1% |

**Detailed Calculations:**

```
k=1â†’2: 
Absolute: 55.0 - 0 = 55.0 percentage points
Relative: (55.0 - 0) / 0 = undefined (from zero)

k=2â†’3:
Absolute: 85.0 - 55.0 = 30.0 percentage points
Relative: 30.0 / 55.0 = 0.545 = 54.5% improvement

k=3â†’4:
Absolute: 87.0 - 85.0 = 2.0 percentage points
Relative: 2.0 / 85.0 = 0.024 = 2.4% improvement

k=4â†’5:
Absolute: 88.5 - 87.0 = 1.5 percentage points
Relative: 1.5 / 87.0 = 0.017 = 1.7% improvement

k=5â†’6:
Absolute: 89.5 - 88.5 = 1.0 percentage points
Relative: 1.0 / 88.5 = 0.011 = 1.1% improvement
```

**Key Observation:**

```
DRAMATIC DROP at k=3â†’4:
- Before k=3: Increases of 55% and 30% (large!)
- After k=3: Increases of 2%, 1.5%, 1.0% (tiny!)

This confirms k=3 as the elbow point!
```

[2 marks: 1 for calculations, 1 for observations]

---

**(d) Recommend optimal k with justification [1 mark]**

**RECOMMENDATION: k = 3** âœ“

**Justification:**

**1. RÂ² Value at k=3:**
```
RÂ² = 85%
- Explains 85% of total variance
- Generally, RÂ² > 80% is considered good
- Acceptable for most applications
```

**2. Diminishing Returns:**
```
Further increases provide minimal benefit:
- k=3â†’4: Only 2% improvement (85% â†’ 87%)
- k=4â†’5: Only 1.5% improvement
- k=5â†’6: Only 1% improvement

To gain 5% more variance explained (85%â†’90%),
we'd need to go from k=3 to k=6+
That's 3+ additional clusters for marginal gain!
```

**3. Rate of Change Analysis:**
```
Percentage increases:
k=1â†’2: 55% (huge)
k=2â†’3: 30% (large)
k=3â†’4: 2% (tiny) â† Sharp drop!
k=4â†’5: 1.5% (tiny)

The sharp drop at k=3â†’4 indicates
k=3 captures the natural structure
```

**4. Parsimony Principle:**
```
Prefer simpler models when performance is similar
k=3 provides 85% variance explained
k=6 provides 89.5% variance explained

Is 4.5% improvement worth 3 extra clusters?
Usually NO!

3 clusters are:
- Easier to interpret
- Easier to communicate
- Less likely to overfit
- More actionable for business
```

**5. Cost-Benefit:**
```
Cost of extra clusters:
- More complex segmentation strategy
- Harder to explain to stakeholders
- More customer segments to manage
- Potential overfitting

Benefit at k=3:
- 85% variance explained
- Clear improvement from k=2 (30% jump)
- Natural structure likely captured
- Actionable number of segments
```

**FINAL RECOMMENDATION:**

```
Use k = 3 clusters because:
âœ“ RÂ² = 85% (good threshold reached)
âœ“ Clear elbow point (rate of improvement drops 15Ã—)
âœ“ Parsimonious (simple enough to be useful)
âœ“ Natural structure captured
âœ“ Diminishing returns beyond k=3

Alternative: If RÂ² > 87% is critical, could consider k=4
But k=3 is the optimal balance of quality vs. complexity!
```

[1 mark for recommendation and complete justification]

---

This completes the comprehensive coverage of **METHOD 4: COHESION AND SEPARATION (WSS/BSS)**.

